{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "381b1de8-172b-48c6-8563-83271fd8fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import IPython.display as display\n",
    "from collections.abc import Iterable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c9f8630-1bb9-41a2-bb41-8ea7ecfe274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize knowledge base (a dictionary)\n",
    "knowledge_base = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727fa32b-f43d-463b-be45-1e155c7546d8",
   "metadata": {},
   "source": [
    "### Loading and Saving the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0276be2b-a58b-4bb1-94b0-66f9317aae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading knowledge from file\n",
    "filename = os.path.join(\".\", \"ai_knowledge.json\")\n",
    "\n",
    "def load_knowledge():\n",
    "    global knowledge_base\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            knowledge_base = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        knowledge_base = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b89cbff-1b6b-4731-a16e-8d1205336e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save knowledge to file\n",
    "def save_knowledge(filename=os.path.join(\".\", \"ai_knowledge.json\")):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(knowledge_base, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d2980-fb54-45eb-9b00-6b44f1437e37",
   "metadata": {},
   "source": [
    "### Translate the content from URL into Ensligh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67bbaa58-cf64-4be8-ade3-942aae120e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the content from URL into English\n",
    "\"\"\"from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def translate_to_english(text):\n",
    "    #Translate non-English text to English while preserving English parts.\n",
    "    try:\n",
    "        # Split the text into words and identify non-English parts\n",
    "        words = text.split()\n",
    "        translated_parts = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Detect the language of each word\n",
    "            lang = translator.detect(word).lang\n",
    "            # If the word is not English, translate it\n",
    "            if lang != 'en':\n",
    "                translated = translator.translate(word, dest='en')\n",
    "                translated_parts.append(translated.text)\n",
    "            else:\n",
    "                translated_parts.append(word)  # Keep English words as they are\n",
    "        \n",
    "        # Join the translated parts back into a single string\n",
    "        return ' '.join(translated_parts)\n",
    "    except Exception as e:\n",
    "        return f\"Error during translation: {str(e)}\"\"\"\n",
    "\n",
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def translate_to_english(text):\n",
    "    \"\"\"Translate non-English text to English while preserving English parts.\"\"\"\n",
    "    try:\n",
    "        # Detect the language of the entire text\n",
    "        detected_language = translator.detect(text).lang\n",
    "        \n",
    "        # If the text is not in English, translate it\n",
    "        if detected_language != 'en':\n",
    "            translation = translator.translate(text, dest='en')\n",
    "            return translation.text\n",
    "        else:\n",
    "            return text  # If the text is already in English, return it as is\n",
    "    except Exception as e:\n",
    "        return f\"Error during translation: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef81b5c9-d4e4-4581-8136-c4e327424366",
   "metadata": {},
   "source": [
    "### Function to Fetch content from URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b077aceb-a362-4152-9374-e959f5898a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch content from url\n",
    "def fetch_content_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Use a list of common tags for extracting meaningful content\n",
    "        common_tags = [\n",
    "            'p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', \n",
    "            'span', 'strong', 'em', 'blockquote', 'li', \n",
    "            'div', 'article', 'section', 'main', 'header', 'footer'\n",
    "        ]\n",
    "        content_parts = []\n",
    "\n",
    "        # Iterate through the common tags and extract text\n",
    "        for tag in common_tags:\n",
    "            elements = soup.find_all(tag)\n",
    "            for element in elements:\n",
    "                text = element.get_text(strip=True)\n",
    "                if text:  # Only add non-empty text\n",
    "                    content_parts.append(text)\n",
    "\n",
    "        # Join the extracted content into a single string\n",
    "        content = \"\\n\\n\".join(content_parts)\n",
    "        print(content)\n",
    "        # Return content or error message if no content found\n",
    "        return content[:2000] if content else \"Error: Could not extract meaningful content.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching content from {url}: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043c00e4-1c3d-41d5-be6d-a896b1bc95a3",
   "metadata": {},
   "source": [
    "### Training Model to sort the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "070344fd-c667-4c37-8dc0-67f55b8d4934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838f61a-a938-4262-857f-eee37350f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the dataset is placed in a 'data' folder within your project directory\n",
    "labeled_dataset_path = os.path.join(\".\", \"data\", \"Cleaned_output_file.xlsx\")\n",
    "\n",
    "# Assuming the model file is placed in a 'models' folder within your project directory\n",
    "model_filename = os.path.join(\".\", \"models\", \"voting_classifier_model.joblib\")\n",
    "\n",
    "# Load the dataset\n",
    "labeled_dataset = pd.read_excel(labeled_dataset_path)\n",
    "\n",
    "# Updated train_model function with ensemble method\n",
    "def train_model():\n",
    "    X = labeled_dataset['Sentences']\n",
    "    y = labeled_dataset['Labels']\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create individual models\n",
    "    model1 = LogisticRegression(max_iter=1000)\n",
    "    model2 = SVC(probability=True)  # Enable probability estimates\n",
    "    model3 = RandomForestClassifier()\n",
    "\n",
    "    # Combine models into a voting classifier\n",
    "    ensemble_model = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('logistic', model1),\n",
    "            ('svc', model2),\n",
    "            ('random_forest', model3)\n",
    "        ],\n",
    "        voting='soft'  # Use 'soft' for probability-based voting\n",
    "    )\n",
    "\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "        ('classifier', ensemble_model)\n",
    "    ])\n",
    "\n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_predict = pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "\n",
    "    # Save the model to disk\n",
    "    joblib.dump(pipeline, model_filename)\n",
    "\n",
    "    return pipeline, accuracy\n",
    "\n",
    "def load_model():\n",
    "    if os.path.exists(model_filename):\n",
    "        # Load the model from disk\n",
    "        pipeline = joblib.load(model_filename)\n",
    "        return pipeline, None  # Return None for accuracy since we're not evaluating it here\n",
    "    else:\n",
    "        return train_model()\n",
    "\n",
    "# Use the model\n",
    "pipeline, accuracy = load_model()\n",
    "if accuracy is not None:\n",
    "    print(f'Model accuracy: {accuracy:.2f}')\n",
    "else:\n",
    "    print(\"Model loaded from disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a20a81b-65a5-454d-b9d0-6fafb3b394c6",
   "metadata": {},
   "source": [
    "### Sorting Useful Information, got from the website URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61071ea-4706-48a4-8a25-2fa1acc5f7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sorting information function\n",
    "def sorting_information(raw_data):\n",
    "    model, accuracy = load_model()\n",
    "    if accuracy is not None:\n",
    "        print(\"Model is trained\")\n",
    "    else:\n",
    "        print(\"Model loaded from disk\")\n",
    "    sentences = raw_data.split('. ')\n",
    "    \n",
    "    # Filter out useful information\n",
    "    useful_sentences = []\n",
    "    for sentence in sentences:\n",
    "        prediction = model.predict([sentence])[0]\n",
    "        #print(f\"Sentence: {sentence.strip()} | Prediction: {prediction}\")  # Debugging line\n",
    "        if prediction == 'Useful':  # Make sure to match the label\n",
    "            useful_sentences.append([sentence.strip()])\n",
    "\n",
    "    # Join all the useful sentences into a single text\n",
    "    #useful_text = '\\n\\n'.join(useful_sentences)\n",
    "    useful_text = '\\n\\n'.join([item[0] for item in useful_sentences])\n",
    "    useful_text = useful_text.replace('[Tex]', '$$').replace('[/Tex]', '$$')\n",
    "    \n",
    "    # Add the new useful sentences to the dataset (label them as 'Useful')\n",
    "    new_data = pd.DataFrame({'Sentences': [item[0] for item in useful_sentences], 'Labels': ['Useful'] * len(useful_sentences)})\n",
    "    updated_data = pd.concat([labeled_dataset, new_data], ignore_index=True)\n",
    "\n",
    "    # Save the updated dataset back to the excel file\n",
    "    updated_data.to_excel(labeled_dataset_path, index=False)\n",
    "\n",
    "    return useful_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e816c-07a3-4d57-851d-e9b9855768f8",
   "metadata": {},
   "source": [
    "### Function for AI to ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e93856-1a7f-40f3-87ac-87a7061776e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_input():\n",
    "    \"\"\"Function to get user input for a topic.\"\"\"\n",
    "    user_input = input(\"What topic do you want to add information about? \")\n",
    "\n",
    "    # Store the answer under a common response\n",
    "    if user_input not in knowledge_base:\n",
    "        knowledge_base[user_input] = []\n",
    "\n",
    "    while True:\n",
    "        main_point = input(\"Enter a main point (or type 'stop' to finish): \")\n",
    "        if main_point.lower() == 'stop':\n",
    "            break\n",
    "        \n",
    "        # Initialize an empty list to hold comments for this main point\n",
    "        comments = []\n",
    "        \n",
    "        while True:\n",
    "            comment = input(f\"Enter a comment for '{main_point}' (or type 'stop' to finish): \")\n",
    "            if comment.lower() == 'stop':\n",
    "                break\n",
    "            comments.append(comment.strip())\n",
    "        \n",
    "        # Store the main point and its corresponding comments\n",
    "        knowledge_base[user_input].append({\n",
    "            \"main_point\": main_point.strip(),\n",
    "            \"comments\": comments\n",
    "        })\n",
    "\n",
    "    print(f\"AI: I learned your answer about '{user_input}'\")\n",
    "\n",
    "    # Save the updated knowledge base\n",
    "    save_knowledge()\n",
    "\n",
    "# Function for AI to ask a question\n",
    "def ask_ai_question(question):\n",
    "    \"\"\"Ask a question to the AI and store the answer.\"\"\"\n",
    "    # Checking if AI already knows the answer \n",
    "    if question in knowledge_base:\n",
    "        return f\"{knowledge_base[question]}\"\n",
    "    else:\n",
    "        # AI doesn't know the answer, ask the user to provide the answer\n",
    "        print(f\"AI: I don't know the answer to '{question}'. What should I learn?\")\n",
    "        user_input = input(\"Provide an answer or a URL for me to learn (or type 'skip' to skip): \")\n",
    "\n",
    "        if user_input.lower() == 'skip':\n",
    "            print(\"AI: You can enter the answer later.\")\n",
    "            return  # Exit from the answer input state\n",
    "\n",
    "        if user_input.startswith('https://') or user_input.startswith('http://'):\n",
    "            # Learn from the URL\n",
    "            content = fetch_content_from_url(user_input)\n",
    "            if \"error\" in content.lower():\n",
    "                print(f\"Error in the fetched content: {content}\")\n",
    "                return\n",
    "            #translated_content = translate_to_english(content)\n",
    "            #if \"error\" in translated_content.lower():\n",
    "                #print(f\"Error in translated content: {translated_content}\")\n",
    "                #return\n",
    "\n",
    "            # Sorting useful information from the data\n",
    "            sorted_information = sorting_information(content)\n",
    "            print(\"Fetched and sorted information:\")\n",
    "            print(sorted_information) \n",
    "            \n",
    "            if input(\"Is this information useful? (y/n): \").lower() == 'y':\n",
    "                # Store the answer under a common response\n",
    "                if sorted_information not in knowledge_base.values():\n",
    "                    knowledge_base[question] = sorted_information\n",
    "                print(f\"AI: I learned from the website: {user_input}\")\n",
    "                save_knowledge()\n",
    "                return sorted_information\n",
    "            else:\n",
    "                print(\"AI: The information was not saved.\")\n",
    "                return False\n",
    "\n",
    "            # Sorting useful information from bunch of data\n",
    "            #translated_information = sorting_information(translated_content)\n",
    "            \n",
    "            # Store the answer under a common response\n",
    "            #if translated_information not in knowledge_base.values():\n",
    "            #    knowledge_base[question] = translated_information\n",
    "            \n",
    "            #knowledge_base[question].append(translated_information)  # Append the URL as a source\n",
    "           # print(f\"AI: I learned from the website: {user_input}\")\n",
    "            #save_knowledge()\n",
    "            #return f\"{translated_information}\" \n",
    "        else:\n",
    "            get_user_input()\n",
    "            return \"I learned your answer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065855a-aa7c-4312-b6c5-8a3cade86d48",
   "metadata": {},
   "source": [
    "### Functions to format, wrap and show all the QUESTIONS/ANSWERS and delete them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92012cd-0b82-4183-b049-bb66692d3fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def wrap_text(text, width=80):\n",
    "    \"\"\"Wrap the text to the specified width.\"\"\"\n",
    "    return textwrap.fill(text, width)\n",
    "\n",
    "def format_answer(answer):\n",
    "    \"\"\"Format the answer in bold.\"\"\"\n",
    "    return f\"\\033[1m{answer}\\033[0m\"\n",
    "\n",
    "def format_data(raw_data): \n",
    "    if isinstance(raw_data, list):\n",
    "        # If the input is a list, format each item\n",
    "        formatted_data = \"\\n\\n\".join(format_data(item) for item in raw_data)\n",
    "        return formatted_data\n",
    "    \n",
    "    # Rest of the function remains the same\n",
    "    paragraphs = raw_data.split(\"\\n\")\n",
    "    \n",
    "    # Process each paragraph\n",
    "    formatted_paragraphs = []\n",
    "    for para in paragraphs:\n",
    "        # Remove excessive whitespace\n",
    "        para = re.sub(r'\\s+', ' ', para).strip()\n",
    "        \n",
    "        # Add bullet points for lists\n",
    "        if para.startswith('•'):\n",
    "            para = f\"- {para[1:].strip()}\"\n",
    "        \n",
    "        # Capitalize first letter of each sentence\n",
    "        para = re.sub(r'(?<=\\. )(?=[a-z])', lambda x: x.group(0).upper(), para)\n",
    "        \n",
    "        # Add formatted paragraph to list\n",
    "        if para:\n",
    "            formatted_paragraphs.append(para)\n",
    "    \n",
    "    # Join paragraphs with line breaks\n",
    "    formatted_data = \"\\n\\n\".join(formatted_paragraphs)\n",
    "    \n",
    "    # Add a header\n",
    "    header = \"Formatted Data Output:\\n\" + \"=\"*30 + \"\\n\"\n",
    "    \n",
    "    return header + formatted_data\n",
    "\n",
    "def show_all_answers():\n",
    "    \"\"\"Print all stored questions with their answers.\"\"\"\n",
    "    if knowledge_base:\n",
    "        print(\"Stored Questions and Answers:\")\n",
    "        for questions, answer in knowledge_base.items():\n",
    "            print(f\"Question {questions} \\nAnswer: {answer}\")#\\nDetails: {data.get('details', 'No additional details provided.')}\\n\")\n",
    "    else:\n",
    "        print(\"AI: No questions and answers have been learned yet.\")\n",
    "\n",
    "def show_all_questions():\n",
    "    \"\"\"Print all stored questions\"\"\"\n",
    "    if knowledge_base:\n",
    "        print(\"Stored Questions:\")\n",
    "        for questions in knowledge_base.keys():\n",
    "            print(f\"Question: {questions}\")\n",
    "    else:\n",
    "        print(\"AI: No questions have been learned yet.\")\n",
    "\n",
    "def remove_ai_memory(question):\n",
    "    # Check if the question exists in knowledge_base\n",
    "    if knowledge_base:\n",
    "        if question in knowledge_base:\n",
    "            del knowledge_base[question] # Remove the question\n",
    "            save_knowledge()\n",
    "            print(f\"AI: The Memory for the question '{question}' has been removed.\")\n",
    "        else:\n",
    "            print(f\"AI: The question '{question}' was not found in memory.\")\n",
    "    else:\n",
    "        print(\"AI: No questions have been learned yet.\")\n",
    "\n",
    "# Removing all the knowledge\n",
    "def remove_all_knowledge():\n",
    "    global knowledge_base\n",
    "    if knowledge_base:\n",
    "        knowledge_base.clear()\n",
    "        save_knowledge()# Clear the knowledge_base dictionary\n",
    "        print(\"AI: All questions and answers have been removed from memory.\")\n",
    "    else:\n",
    "        print(\"AI: No questions have been learned yet.\")\n",
    "\n",
    "import re\n",
    "\n",
    "# Helper function to flatten a nested list\n",
    "def flatten(lst):\n",
    "    for i in lst:\n",
    "        if isinstance(i, Iterable) and not isinstance(i, str):\n",
    "            yield from flatten(i)\n",
    "        else:\n",
    "            yield i\n",
    "\n",
    "def showing_mathematical_equations(raw_text):\n",
    "    # If the input is a list (or nested list), flatten and join into a single string\n",
    "    if isinstance(raw_text, list):\n",
    "        raw_text = ' '.join(flatten(raw_text))\n",
    "    \n",
    "    # Clean up the raw text and handle LaTeX-like expressions\n",
    "    cleaned_text = re.sub(r'\\[Tex\\](.*?)\\[/Tex\\]', r'$\\1$', raw_text, flags=re.DOTALL)\n",
    "    \n",
    "    # Optionally handle newlines and extra spaces if needed\n",
    "    cleaned_text = re.sub(r'\\n+', '\\n', cleaned_text)  # Clean multiple newlines\n",
    "    cleaned_text = cleaned_text.strip()  # Remove leading/trailing spaces\n",
    "    print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8094bea-fdce-4eac-b0d6-b24a7b5b1253",
   "metadata": {},
   "source": [
    "### Function to check if question already exists in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bd2082-719c-43df-9ebd-da3f6ab51242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the pre-trained sentence-transformer model once\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Precompute embeddings for the stored questions\n",
    "def precompute_question_embeddings(knowledge_base):\n",
    "    stored_questions = list(knowledge_base.keys())\n",
    "    stored_question_embeddings = model.encode(stored_questions, convert_to_tensor=True)\n",
    "    return stored_question_embeddings, stored_questions\n",
    "\n",
    "# Precompute the embeddings once\n",
    "\n",
    "\n",
    "def is_question_in_memory(user_question):\n",
    "    if not knowledge_base:\n",
    "        return False\n",
    "    # Precompute the embeddings once\n",
    "    stored_question_embeddings, stored_questions = precompute_question_embeddings(knowledge_base)\n",
    "    \n",
    "    # Encode the user question   \n",
    "    user_question_embedding = model.encode(user_question, convert_to_tensor=True)\n",
    "\n",
    "    # Calculate the similarity between the user question and stored questions\n",
    "    similarities = util.pytorch_cos_sim(user_question_embedding, stored_question_embeddings)\n",
    "\n",
    "    # Find the maximum similarity score and its index\n",
    "    max_similarity = similarities.max().item()\n",
    "    max_index = similarities.argmax().item()\n",
    "\n",
    "    # Define a similarity threshold (e.g., 0.8 for paraphrased questions)\n",
    "    threshold = 0.8\n",
    "    if max_similarity > threshold:\n",
    "        # Retrieve the corresponding question from the knowledge base\n",
    "        matched_question = stored_questions[max_index]\n",
    "        answer = knowledge_base[matched_question]\n",
    "        return answer  # Return the answer and True indicating the question exists\n",
    "    else:\n",
    "        return False  # Return False, False indicating the question is not found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e72343-599f-43ac-a418-26a0cdab738b",
   "metadata": {},
   "source": [
    "### Voice Recognition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf5f897-d3db-42a3-849f-b9692327d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "def recognize_until_stop():\n",
    "    recognizer = sr.Recognizer()\n",
    "    \n",
    "    # Use the microphone as the audio source\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Adjusting for ambient noise. Please wait...\")\n",
    "        recognizer.adjust_for_ambient_noise(source, duration=1)\n",
    "        print(\"Listening... (Say 'stop' to end)\")\n",
    "\n",
    "        collected_text = []  # To collect the spoken text\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                # Listen for audio\n",
    "                audio_data = recognizer.listen(source)\n",
    "                print(\"Recognizing...\")\n",
    "                \n",
    "                # Recognize speech using Google Web Speech API\n",
    "                text = recognizer.recognize_google(audio_data)\n",
    "                print(f\"You said: {text}\")\n",
    "\n",
    "                if text.lower() == \"stop\":\n",
    "                    print(\"Stopping...\")\n",
    "                    break  # Exit the loop if \"stop\" is said\n",
    "\n",
    "                # Append the recognized text\n",
    "                collected_text.append(text)\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Sorry, I could not understand the audio.\")\n",
    "            except sr.RequestError as e:\n",
    "                print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "        # Return the collected text as a single string\n",
    "        return \" \".join(collected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343c028e-dd02-4620-8ace-f6d46da2cf0e",
   "metadata": {},
   "source": [
    "### Providing google search links for user quesiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f050db1-0bcd-47f9-8dab-2ccc1985a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote_plus\n",
    "\n",
    "def save_to_json(data, filename='search_results.json'):\n",
    "    \"\"\"Save the data to a JSON file.\"\"\"\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "def load_from_json(filename='search_results.json'):\n",
    "    \"\"\"Load the data from a JSON file.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as json_file:\n",
    "            return json.load(json_file)\n",
    "    return {}\n",
    "\n",
    "def search_internet(user_input):\n",
    "    \"\"\"Search for the user input on Google and return the first five links.\"\"\"\n",
    "    search_url = f\"https://www.google.com/search?q={quote_plus(user_input)}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract search results\n",
    "        results = soup.find_all('h3')  # Look for the <h3> tags which usually contain the titles\n",
    "        links = []\n",
    "        \n",
    "        for result in results:\n",
    "            a_tag = result.find_parent('a')  # Find the parent anchor tag to get the link\n",
    "            if a_tag and a_tag.get('href'):\n",
    "                link = a_tag.get('href')\n",
    "                # Clean the link by splitting to remove unnecessary parameters\n",
    "                if link.startswith('/url?q='):\n",
    "                    clean_link = link.split('/url?q=')[1].split('&')[0]  # Get the actual URL\n",
    "                    links.append(clean_link)\n",
    "                elif 'http' in link:  # Directly add if it's already a valid URL\n",
    "                    links.append(link)\n",
    "            if len(links) >= 5:  # Stop after finding 5 links\n",
    "                break\n",
    "\n",
    "        return links\n",
    "    else:\n",
    "        print(f\"HTTP Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def provide_links(user_input):\n",
    "    # Load existing search results from JSON file\n",
    "    search_history = load_from_json()\n",
    "\n",
    "    # Check if the question has already been asked\n",
    "    if user_input in search_history:\n",
    "        print(\"Here are the saved links for your question:\")\n",
    "        for index, link in enumerate(search_history[user_input], start=1):\n",
    "            print(f\"{index}. {link}\")\n",
    "    else:\n",
    "        # Search the internet\n",
    "        print(\"Searching the internet...\")\n",
    "        links = search_internet(user_input)\n",
    "\n",
    "        if links:\n",
    "            print(\"Here are the first five links:\")\n",
    "            for index, link in enumerate(links, start=1):\n",
    "                print(f\"{index}. {link}\")\n",
    "            \n",
    "            # Save to search history\n",
    "            search_history[user_input] = links\n",
    "            save_to_json(search_history)\n",
    "        else:\n",
    "            print(\"Error: Unable to retrieve search results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2935a05a-9fcb-4d44-91e8-3c66b630186e",
   "metadata": {},
   "source": [
    "### Main loop to interact with the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76815334-49f2-4c62-b459-23760c52b399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop to interact with the AI\n",
    "def main():\n",
    "    load_knowledge()\n",
    "    \n",
    "    print(\"Welcome! You can ask me questions or describe what you're doing.\")\n",
    "    user_choice = input(\"Do you want to use voice recognition (y/n): \").lower()\n",
    "    while True:\n",
    "        if user_choice == 'y':\n",
    "            question = recognize_until_stop()\n",
    "        else:\n",
    "            question = input(\"\\nAsk a question or type 'exit' to stop: \").lower()\n",
    "            \n",
    "        if question == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        if question == 'show questions' or question == 'show all the questions' or question == 'show all questions':\n",
    "            show_all_questions()\n",
    "            continue  # Skip further processing and go back to the main loop\n",
    "\n",
    "        if question == 'show answers' or question == 'show all the answers' or question == 'show all answers':\n",
    "            show_all_answers()\n",
    "            continue  # Skip further processing and go back to the main loop\n",
    "\n",
    "        if question == 'remove question' or question == 'delete question' or question == 'remove the answer' or question == 'delete answer' or question == 'delete the answer':\n",
    "            if input(\"Are you sure? y/n\").lower() == 'y':\n",
    "                question_to_remove = input(\"Enter the question to remove --> \")\n",
    "                remove_ai_memory(question_to_remove)\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        if question == 'remove all questions' or question == 'remove all question' or question == 'remove all answers':\n",
    "            if input(\"Are you sure? Doing this will wipe out all the memory of AI (y/n): \") == 'y':\n",
    "                if input(\"Please confirm (y/n): \") == 'y':\n",
    "                    remove_all_knowledge()\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        user_question = is_question_in_memory(question)\n",
    "        if user_question:\n",
    "            print(f\"AI Answer:\")\n",
    "            formated_question = format_data(user_question)\n",
    "            showing_mathematical_equations(formated_question)\n",
    "            continue\n",
    "\n",
    "        links = input(\"Do you want me to provide you links for your question (y/n): \").lower()\n",
    "        if links == 'y':\n",
    "            provide_links(question)\n",
    "            continue\n",
    "\n",
    "        answer = ask_ai_question(question)\n",
    "        if answer:\n",
    "            #bold_answer = format_answer(answer)\n",
    "            #wrapped_answer = wrap_text(bold_answer, width=80)\n",
    "            formated_data = format_data(answer)\n",
    "            showing_mathematical_equations(formated_data)\n",
    "            #print(f\"AI Answer: {wrapped_answer}\")\n",
    "        else:\n",
    "            print(\"No change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec93577-f912-4d02-9d20-09197357cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
